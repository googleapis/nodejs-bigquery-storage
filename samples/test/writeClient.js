// Copyright 2022 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// ** This file is automatically generated by gapic-generator-typescript. **
// ** https://github.com/googleapis/gapic-generator-typescript **
// ** All changes to this file may be overwritten. **

'use strict';

const {assert} = require('chai');
const {describe, it, before, after} = require('mocha');
const uuid = require('uuid');
const cp = require('child_process');
const {BigQuery} = require('@google-cloud/bigquery');

const bigquery = new BigQuery();

const execSync = cmd => cp.execSync(cmd, {encoding: 'utf-8'});

const GCLOUD_TESTS_PREFIX = 'nodejs_bqstorage_samples_test';

const generateUuid = () =>
  `${GCLOUD_TESTS_PREFIX}_${uuid.v4()}`.replace(/-/gi, '_');

const datasetId = generateUuid();

describe('writeClient', () => {
  let projectId;

  before(async () => {
    await deleteDatasets();

    await bigquery.createDataset(datasetId);
  });

  after(async () => {
    await bigquery.dataset(datasetId).delete({force: true}).catch(console.warn);
  });

  it('should append rows', async () => {
    const schema = [{name: 'customer_name', type: 'STRING'}];

    const tableId = generateUuid();

    const [table] = await bigquery
      .dataset(datasetId)
      .createTable(tableId, {schema});

    projectId = table.metadata.tableReference.projectId;

    const output = execSync(
      `node append_rows_pending ${projectId} ${datasetId} ${tableId}`
    );
    assert.match(output, /Stream created:/);
    assert.match(output, /Row count: 3/);
    let [rows] = await table.query(
      `SELECT * FROM \`${projectId}.${datasetId}.${tableId}\``
    );
    rows = rows.map(row => {
      if (row.customer_name !== null) {
        return row;
      }
    });

    assert.strictEqual(rows.length, 3);
    assert.deepInclude(rows, {customer_name: 'Octavia'});
    assert.deepInclude(rows, {customer_name: 'Turing'});
    assert.deepInclude(rows, {customer_name: 'bell'});
  });

  it('should append rows with multiple types', async () => {
    const schema = [
      {name: 'bool_col', type: 'BOOLEAN'},
      {name: 'bytes_col', type: 'BYTES'},
      {name: 'float64_col', type: 'FLOAT'},
      {name: 'int64_col', type: 'INTEGER'},
      {name: 'string_col', type: 'STRING'},
      {name: 'date_col', type: 'DATE'},
      {name: 'datetime_col', type: 'DATETIME'},
      {name: 'geography_col', type: 'GEOGRAPHY'},
      {name: 'numeric_col', type: 'NUMERIC'},
      {name: 'bignumeric_col', type: 'BIGNUMERIC'},
      {name: 'time_col', type: 'TIME'},
      {name: 'timestamp_col', type: 'TIMESTAMP'},
      {name: 'int64_list', type: 'INTEGER', mode: 'REPEATED'},
    ];

    const tableId = generateUuid();

    const [table] = await bigquery
      .dataset(datasetId)
      .createTable(tableId, {schema});

    projectId = table.metadata.tableReference.projectId;

    const output = execSync(
      `node append_rows_proto2 ${projectId} ${datasetId} ${tableId}`
    );
    assert.match(output, /Stream created:/);
    assert.match(output, /Row count: 13/);
    let [rows] = await table.query(
      `SELECT * FROM \`${projectId}.${datasetId}.${tableId}\``
    );

    rows = rows.map(row => {
      return Object.entries(row)
        .filter(([, value]) => value !== null && value.length !== 0)
        .map(([name, value]) => {
          if (value.value) {
            value = value.value;
          }
          if (name === 'numeric_col' || name === 'bignumeric_col') {
            value = value.toNumber();
          }
          return {[name]: value};
        });
    });

    assert.strictEqual(rows.length, 13);
    assert.deepInclude(rows, [
      {
        bool_col: true,
      },
      {bytes_col: Buffer.from('hello world')},
      {float64_col: 123.44999694824219},
      {int64_col: 123},
      {string_col: 'omfg!'},
    ]);
    assert.deepInclude(rows, [{bool_col: false}]);
    assert.deepInclude(rows, [{bytes_col: Buffer.from('later, gator')}]);
    assert.deepInclude(rows, [{float64_col: 987.6539916992188}]);
    assert.deepInclude(rows, [{int64_col: 321}]);
    assert.deepInclude(rows, [{string_col: 'octavia'}]);
    assert.deepInclude(rows, [{date_col: '5071-10-07'}]);
    assert.deepInclude(rows, [{datetime_col: '2019-02-17T11:24:00'}]);
    assert.deepInclude(rows, [{geography_col: 'POINT(5 5)'}]);
    assert.deepInclude(rows, [{numeric_col: 123456}, {bignumeric_col: 1e29}]);
    assert.deepInclude(rows, [{time_col: '18:00:00'}]);
    assert.deepInclude(rows, [{timestamp_col: '1970-01-20T00:01:40.186Z'}]);
    assert.deepInclude(rows, [{int64_list: [1999, 2001]}]);
  });

  // Only delete a resource if it is older than 24 hours. That will prevent
  // collisions with parallel CI test runs.
  function isResourceStale(creationTime) {
    const oneDayMs = 86400000;
    const now = new Date();
    const created = new Date(creationTime);
    return now.getTime() - created.getTime() >= oneDayMs;
  }

  async function deleteDatasets() {
    let [datasets] = await bigquery.getDatasets();
    datasets = datasets.filter(dataset =>
      dataset.id.includes(GCLOUD_TESTS_PREFIX)
    );

    for (const dataset of datasets) {
      const [metadata] = await dataset.getMetadata();
      const creationTime = Number(metadata.creationTime);

      if (isResourceStale(creationTime)) {
        try {
          await dataset.delete({force: true});
        } catch (e) {
          console.log(`dataset(${dataset.id}).delete() failed`);
          console.log(e);
        }
      }
    }
  }
});
